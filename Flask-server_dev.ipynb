{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f20abde9-3ed7-4cfc-a267-850ad7cbdec7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "====================================================================================================\n",
    "注意由于子文件的文件层级不同 以下的文件路径均有改动 ..->."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1de9d2b-58e8-440a-8135-4acfd45509b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 结果还行！！！！！初版完成！！！！\n",
    "# FIX：3.25 归一化后是乘以100！\n",
    "# Fix:3.29 mbert+bilstm+SelfAttention版本\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "\n",
    "def DateDataProcess_Impl(date_sel):\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    from transformers import BertTokenizer, BertForSequenceClassification\n",
    "    from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "    import torch.nn as nn\n",
    "    import os\n",
    "    # 定义Self-Attention层\n",
    "    class SelfAttention(nn.Module):\n",
    "        def __init__(self, hidden_size):\n",
    "            super(SelfAttention, self).__init__()\n",
    "            self.hidden_size = hidden_size\n",
    "            self.projection = nn.Sequential(\n",
    "                nn.Linear(hidden_size, 64),\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(64, 1)\n",
    "            )\n",
    "\n",
    "        def forward(self, encoder_outputs):\n",
    "            energy = self.projection(encoder_outputs)\n",
    "            weights = torch.softmax(energy.squeeze(-1), dim=1)\n",
    "            outputs = (encoder_outputs * weights.unsqueeze(-1)).sum(dim=1)\n",
    "            return outputs\n",
    "\n",
    "\n",
    "        # 定义模型\n",
    "    class NewsClassifier(nn.Module):\n",
    "        # hidden_size = 128\n",
    "        def __init__(self, bert_model,num_classes, hidden_size, num_layers=2, bidirectional=True):\n",
    "            super(NewsClassifier, self).__init__()\n",
    "            # self.bert = BertModel.from_pretrained('../../bert-base-multilingual-cased')\n",
    "            self.bert = bert_model # FIX\n",
    "\n",
    "            # self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers, \n",
    "            #                     bidirectional=bidirectional, batch_first=True)\n",
    "            self.lstm = nn.LSTM(input_size=bert_model.config.hidden_size, hidden_size=hidden_size, num_layers=num_layers, \n",
    "                                bidirectional=bidirectional, batch_first=True) # FIX\n",
    "\n",
    "            self.attention = SelfAttention(hidden_size * (2 if bidirectional else 1))\n",
    "            self.fc = nn.Linear(hidden_size * (2 if bidirectional else 1), num_classes)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask):\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "            lstm_outputs, _ = self.lstm(last_hidden_state)\n",
    "            attention_outputs = self.attention(lstm_outputs)\n",
    "            logits = self.fc(attention_outputs)\n",
    "            return logits\n",
    "\n",
    "\n",
    "\n",
    "    # 加载训练好的模型\n",
    "    model_path = './models/bert-base-multilingual-cased'  ## 可更换\n",
    "    # modelNew_load_path = './classificationModel/bert-base-multilingual-cased_classification_undersampled_new_epoch_20.pth'  ## 可更换\n",
    "    # modelNew_load_path = '../NewsAthmTask2Score/classificationModel/bert-base-multilingual-cased_classification_undersampled_new_epoch_20.pth'  ## 可更换\n",
    "    modelNew_load_path = './classificationModel/best_MultiBert_BiLSTM_SelfAttention_modelFIX_fold_5.pth'  ## 可更换\n",
    "\n",
    "    model_CLS_name = \"mbert_BiLSTM_SelfAttention\" ###!!!\n",
    "\n",
    "    # model = BertForSequenceClassification.from_pretrained(model_path, num_labels=9)\n",
    "    model = BertModel.from_pretrained(model_path)\n",
    "\n",
    "    # 加载tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    # 将模型移动到GPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # print(device)\n",
    "    # model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "    print(device)\n",
    "    model.to(device)\n",
    "\n",
    "    # 设置超参数\n",
    "    max_length = 512\n",
    "    hidden_size = 128\n",
    "    num_classes = 9\n",
    "    num_layers = 2\n",
    "    bidirectional = True\n",
    "\n",
    "\n",
    "    # 加载训练好的模型\n",
    "    # modelNew_load_path = '../NewsAthmTask2Score/classificationModel/best_MultiBert_BiLSTM_SelfAttention_modelFIX_fold_5.pth'  ## 可更换\n",
    "    model = NewsClassifier(bert_model = model,num_classes=num_classes, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional)\n",
    "    model.load_state_dict(torch.load(modelNew_load_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 定义类别列表\n",
    "    # categories = ['খেলাধুলা', 'রাজনীতি', 'বিনোদন', 'অর্থনীতি', 'আইন', 'শিক্ষা', 'বিজ্ঞান', 'লাইফস্টাইল', 'অন্যান্য']\n",
    "    categories = ['রাজনীত','লাইফস্টাইল','শিক্ষা','অর্থনীতি','খেলাধুলা','অন্যান্য','বিজ্ঞান','বিনোদন', 'আইন'] # FIX!!!!!!!!!!!!\n",
    "    # [‘政治’、‘生活方式’、‘教育’、‘经济’、‘体育’、‘其他’、‘科学’、‘娱乐’、‘法律’]\n",
    "\n",
    "    # 定义数据处理函数\n",
    "    def preprocess_data(text, tokenizer, max_length):\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        return encoding['input_ids'].to(device), encoding['attention_mask'].to(device)\n",
    "\n",
    "    # 定义预测函数\n",
    "    def predict(model, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "        return preds.item(), categories[preds.item()]\n",
    "        # return preds.item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 读取csv文件\n",
    "    # data = pd.read_csv('./Data231202-231211/Data231202.csv')  ## \n",
    "    # data = pd.read_csv('./datasets/news_20240302_20240311.csv')  ## 对0302-0311这10天进行评估  从数据库爬取（定时任务）--->拿到数据--->根据date筛选\n",
    "    \n",
    "    data_file_path = f'./datasets/mbert_BiLSTM_SelfAttention/{date_sel}/news_{date_sel}.csv'\n",
    "    # data = pd.read_csv('./datasets/news_20240302_20240318.csv')  ## 对0302-0318这进行评估  从数据库爬取（定时任务）--->拿到数据--->根据date筛选\n",
    "    data = pd.read_csv(data_file_path)  ## 对0302-0318这进行评估  从数据库爬取（定时任务）--->拿到数据--->根据date筛选\n",
    "\n",
    "    data['pub_time'] = pd.to_datetime(data['pub_time'])\n",
    "\n",
    "    # date_UNI = '2024-03-12' ###\n",
    "    date_UNI = date_sel ###\n",
    "\n",
    "    # 筛选 pub_time 为 '2024-03-02' 的数据\n",
    "    filtered_data = data[data['pub_time'] == date_UNI]  ## 这个日期是参数！系统端传过来后进行处理 系统传日期---》查询数据库（看是否有缓存。没有的话就现查）---》筛选\n",
    "\n",
    "    # filtered_data.to_csv(\"./test0302.csv\", index=False)\n",
    "\n",
    "    # 显示筛选结果\n",
    "    # print(filtered_data)\n",
    "\n",
    "\n",
    "    nan_check = filtered_data['body'].isna().sum()\n",
    "    nan_check_c = filtered_data['category1'].isna().sum()\n",
    "    print(nan_check)\n",
    "    print(nan_check_c)\n",
    "\n",
    "    filtered_data = filtered_data.dropna(subset=['category1','body'])\n",
    "    nan_check = filtered_data['body'].isna().sum()\n",
    "    nan_check_c = filtered_data['category1'].isna().sum()\n",
    "    print(nan_check)\n",
    "    print(nan_check_c)\n",
    "\n",
    "\n",
    "    processed_data_file_name = f\"./datasets/{model_CLS_name}/news_{date_UNI}_processed_{model_CLS_name}.csv\"\n",
    "    \n",
    "    # FIX:缓存操作 若已有文件则直接读取 否则才进行预测\n",
    "    # 判断文件是否存在\n",
    "    if os.path.exists(processed_data_file_name):\n",
    "        # 如果文件存在，则直接读取数据\n",
    "        processed_data = pd.read_csv(processed_data_file_name)\n",
    "    else:\n",
    "        # 如果文件不存在，则执行处理数据的函数\n",
    "        # processed_data = process_data(data)\n",
    "        # processed_data = process_data(filtered_data)\n",
    "\n",
    "        # FIX:\n",
    "        predicted_categories = []\n",
    "        cnt = 0\n",
    "        for idx, row in filtered_data.iterrows():\n",
    "            cnt+=1\n",
    "            if cnt%200 == 0: \n",
    "                print(\"categoryProcessing\")\n",
    "                print(cnt)\n",
    "            if row['category1'] not in categories:\n",
    "                input_ids, attention_mask = preprocess_data(row['body'], tokenizer, max_length)\n",
    "                pred_id, predicted_category = predict(model, input_ids, attention_mask)\n",
    "                predicted_categories.append(predicted_category)\n",
    "                # predicted_categories_id.append(pred_id)\n",
    "            else:\n",
    "                predicted_categories.append(row['category1'])\n",
    "                # predicted_categories_id.append(row['category1'])\n",
    "\n",
    "        # 将预测后的类别替换原有的category1列\n",
    "        filtered_data['category1'] = predicted_categories\n",
    "        processed_data = filtered_data\n",
    "\n",
    "        # 将处理后的数据保存到文件中\n",
    "        processed_data.to_csv(processed_data_file_name, index=False)\n",
    "\n",
    "    print(\"FINISH!!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # conda angle https://github.com/SeanLee97/AnglE/tree/main\n",
    "    # pip install nltk\n",
    "    # pip install --upgrade pip\n",
    "    # pip install spacy==2.3.5\n",
    "    # pip install bn_core_news_sm-0.1.0.tar.gz\n",
    "    # pip install matplotlib\n",
    "    import pandas as pd\n",
    "    # from sentence_transformers import SentenceTransformer\n",
    "    import numpy as np\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import torch\n",
    "    from transformers import AutoModel, AutoTokenizer\n",
    "    from angle_emb import AnglE\n",
    "\n",
    "    # yes! 聚类评估！！！可跑 TP, FP, TN, FN 得到RI、Precision、Recall、F1，ARI\n",
    "    # update:单个成簇的处理\n",
    "    from itertools import combinations\n",
    "    from math import comb\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    import networkx as nx\n",
    "    from collections import defaultdict\n",
    "    from nltk.tokenize import word_tokenize # 使用NLTK进行分词，根据需要替换为适合孟加拉语的分词方法\n",
    "\n",
    "    import spacy\n",
    "    # from gensim.summarization import keywords\n",
    "    from collections import defaultdict\n",
    "    import bn_core_news_sm\n",
    "    from sklearn.preprocessing import MinMaxScaler # 归一化\n",
    "    import matplotlib.pyplot as plt\n",
    "    # import pytextrank\n",
    "    # =======\n",
    "    # 去除停用词\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "\n",
    "    import string\n",
    "    # ====================\n",
    "\n",
    "\n",
    "    # data_ORI = pd.read_csv('./Data231202-231211/Data231202.csv') # 所有子任务都是使用这个\n",
    "    data_ORI = processed_data\n",
    "\n",
    "    # 使用angle加载\n",
    "    # model_id = '../NewsAthmTask2/models/angle-bert-base-uncased-nli-en-v1' ## 可更换\n",
    "    model_id = './models/angle-bert-base-uncased-nli-en-v1' ## 可更换\n",
    "\n",
    "    angle = AnglE.from_pretrained(model_id, pooling_strategy='cls_avg').cuda()\n",
    "\n",
    "    # 加载数据\n",
    "    data = data_ORI\n",
    "\n",
    "    # 将日期转换为日期时间格式\n",
    "    data['pub_time'] = pd.to_datetime(data['pub_time'])\n",
    "\n",
    "    # 获取唯一日期列表\n",
    "    dates = data['pub_time'].dt.date.unique()\n",
    "\n",
    "\n",
    "    # 定义聚类中心更新函数\n",
    "    def update_cluster_center(cluster):\n",
    "        cluster_embeddings = angle.encode(cluster, to_numpy=True) # 使用angle加载\n",
    "\n",
    "        return np.mean(cluster_embeddings, axis=0)\n",
    "\n",
    "    def get_predicted_clusters(data,threshold):\n",
    "        # 对于每个日期\n",
    "        cluster_results = []\n",
    "        cnt = 0\n",
    "        for date in dates:\n",
    "            print(cnt)\n",
    "            cnt+=1\n",
    "            # 获取该日期的新闻标题\n",
    "            news_data = data[data['pub_time'].dt.date == date]['title'].tolist()\n",
    "            # 获取该日期的新闻正文\n",
    "            # news_data = data[data['pub_time'].dt.date == date]['body'].tolist() # ByBody\n",
    "\n",
    "            embeddings = angle.encode(news_data, to_numpy=True) # 使用angle加载\n",
    "\n",
    "            # 定义当天的簇列表\n",
    "            daily_clusters = []\n",
    "\n",
    "            # 对于每个新闻数据\n",
    "            for i, embedding in enumerate(embeddings):\n",
    "                # 如果簇列表为空，则新开一个簇\n",
    "                if not daily_clusters:\n",
    "                    # daily_clusters.append({'center': embedding, 'members': [news_data[i]]})\n",
    "                    daily_clusters.append({'center': embedding, 'members': [i],'news':[news_data[i]]}) # 改为存index\n",
    "                    continue\n",
    "\n",
    "                # 计算当前数据点与各个簇中心的相似度\n",
    "                similarities = [cosine_similarity([embedding], [cluster['center']])[0][0] for cluster in daily_clusters]\n",
    "\n",
    "                # 找到最大相似度及其对应的簇索引\n",
    "                max_similarity = max(similarities)\n",
    "                max_index = similarities.index(max_similarity)\n",
    "\n",
    "                # 如果最大相似度大于阈值，则将当前数据点加入对应簇，并更新簇中心\n",
    "                if max_similarity > threshold:\n",
    "                    daily_clusters[max_index]['members'].append(i) # 改为存index\n",
    "                    daily_clusters[max_index]['news'].append(news_data[i]) # 改为存index\n",
    "                    daily_clusters[max_index]['center'] = update_cluster_center(daily_clusters[max_index]['news'])\n",
    "                # 否则新开一个簇\n",
    "                else:\n",
    "                    daily_clusters.append({'center': embedding, 'members': [i],'news':[news_data[i]]}) # 改为存index\n",
    "\n",
    "            # 将当天的簇信息添加到结果列表中\n",
    "            cluster_results.append({'date': date, 'clusters': daily_clusters})\n",
    "\n",
    "        predicted_clusters = []\n",
    "        for cluster in cluster_results[0]['clusters']: # 2023-12-02的簇s\n",
    "            clus_index = []\n",
    "            for i in cluster['members']:\n",
    "                clus_index.append(i)\n",
    "            predicted_clusters.append(clus_index)\n",
    "        print(predicted_clusters)\n",
    "\n",
    "        return predicted_clusters\n",
    "\n",
    "    # 设置阈值\n",
    "    threshold = 0.972  ## 可更换\n",
    "    clusters = get_predicted_clusters(data,threshold)\n",
    "\n",
    "    # 创建一个字典，键是语料索引，值是对应的簇大小\n",
    "    index_to_cluster_size = {index: len(cluster) for cluster in clusters for index in cluster}\n",
    "\n",
    "    # 读取语料文件\n",
    "    df = data_ORI\n",
    "\n",
    "    # 新增列clus_news_num，记录每个语料对应的簇的大小\n",
    "    df['T1_clus_news_num'] = df.index.map(index_to_cluster_size)\n",
    "\n",
    "    # 根据簇大小进行排序，并添加排名，相同大小的排名相同\n",
    "    df = df.sort_values(by='T1_clus_news_num', ascending=False)\n",
    "    df['T1_rank'] = df['T1_clus_news_num'].rank(method='min', ascending=False)\n",
    "\n",
    "    # 新增列S_scale，为簇大小的归一化结果\n",
    "    scaler = MinMaxScaler()\n",
    "    df['T1_S_scale'] = scaler.fit_transform(df[['T1_clus_news_num']])\n",
    "\n",
    "    # # 新增列S_score，为S_scale的值乘以20\n",
    "    # df['T1_S_score'] = df['T1_S_scale'] * 20\n",
    "\n",
    "    # 新增列S_score，为S_scale的值乘以20\n",
    "    df['T1_S_score'] = df['T1_S_scale'] * 100\n",
    "\n",
    "    # 新增列index，表示语料原始的坐标\n",
    "    df['T1_ori_indexFrom0'] = df.index\n",
    "\n",
    "    # 只保留需要的列，并保存到新的CSV文件\n",
    "    T1_final_df = df[['id','T1_ori_indexFrom0', 'title', 'body', 'T1_clus_news_num', 'T1_rank','T1_S_scale', 'T1_S_score']]\n",
    "\n",
    "    # 文件保存处理，若有重名文件，则重命名为_{num}  好像并不需要 每天的是固定的 后续可能直接查询就行\n",
    "    # num_file_T1 = 1\n",
    "\n",
    "    # # 检查文件是否存在\n",
    "    # while os.path.exists(T1_file_name):\n",
    "    #     T1_file_name = f\"./T1ClusterScore/T1_{date_UNI}_result_new_{num_file_T1}.csv\"\n",
    "    #     num_file_T1 += 1\n",
    "\n",
    "    T1_file_name = f\"./T1ClusterScore/{model_CLS_name}/T1_{date_UNI}_{model_CLS_name}_result_new.csv\"\n",
    "    T1_final_df.to_csv(T1_file_name, index=False)\n",
    "    print(\"FINISH!\")\n",
    "\n",
    "\n",
    "    # 40个网站的排名以及赋分结果在./T2WebsiteRank/website_Rank_new.csv\n",
    "    # Data231202-231211/Data231202.csv\n",
    "    # 读取Data231202-231211/Data231202.csv，其中的website_id为网站id，现在读取./T2WebsiteRank/website_Rank_new.csv，该文件存有website_id对应的S_task_web，现在需要将Data231202.csv中的每个语料对应的website_id对应的S_task_web新增一列进行存储，然后根据S_task_web进行排序，允许并列，新增rank列，将结果中website_id,title,S_task_web,rank存到新的csv文件\n",
    "\n",
    "    # 读取两个csv文件\n",
    "    data_df = data_ORI\n",
    "    # rank_df = pd.read_csv('./T2WebsiteRank/website_Rank_new.csv')\n",
    "    rank_df = pd.read_csv('./T2WebsiteRank/website_Rank_new_FIX.csv') # FIX\n",
    "\n",
    "\n",
    "    # 将两个DataFrame合并\n",
    "    merged_df = pd.merge(data_df, rank_df, on='website_id')\n",
    "\n",
    "    # 根据S_task_web列进行排序，并添加排名，相同权重的排名相同\n",
    "    merged_df = merged_df.sort_values(by='T2_S_score', ascending=False)\n",
    "    merged_df['T2_rank'] = merged_df['T2_S_score'].rank(method='min', ascending=False)\n",
    "\n",
    "    # 只保留需要的列，并保存到新的CSV文件\n",
    "    T2_final_df = merged_df[['id','website_id', 'title', 'T2_S_score', 'T2_rank']]\n",
    "\n",
    "    T2_file_name = f\"./T2WebsiteRank/{model_CLS_name}/T2_{date_UNI}_{model_CLS_name}_result_new.csv\" ## FIX\n",
    "    # T2_final_df.to_csv('./T2WebsiteRank/Data231202_scoreResult.csv', index=False)\n",
    "    T2_final_df.to_csv(T2_file_name, index=False)\n",
    "\n",
    "\n",
    "\n",
    "    # 读取CSV文件并计算正文长度\n",
    "    df = data_ORI\n",
    "    df['body_len'] = df['body'].apply(lambda x: len(str(x).split()))  # 假设每个单词之间用空格分隔\n",
    "\n",
    "    # 按正文长度进行排序\n",
    "    df = df.sort_values(by='body_len', ascending=False)\n",
    "\n",
    "    # 添加排名列\n",
    "    df['T3_rank'] = df['body_len'].rank(method='min', ascending=False)\n",
    "\n",
    "    # 计算S_scale并添加列\n",
    "    max_len = df['body_len'].max()\n",
    "    min_len = df['body_len'].min()\n",
    "    df['T3_S_scale'] = (df['body_len'] - min_len) / (max_len - min_len)\n",
    "\n",
    "    # # 计算body_len_score并添加列\n",
    "    # df['T3_S_score'] = 20 * df['T3_S_scale']\n",
    "\n",
    "    # 计算body_len_score并添加列\n",
    "    df['T3_S_score'] = 100 * df['T3_S_scale'] #FIX\n",
    "\n",
    "    # 保存结果到新的CSV文件\n",
    "    T3_file_name_1 = f\"./T3BodyLenRank/{model_CLS_name}/T3_{date_UNI}_{model_CLS_name}_result_new_all.csv\"\n",
    "    T3_file_name_2 = f\"./T3BodyLenRank/{model_CLS_name}/T3_{date_UNI}_{model_CLS_name}_result_new.csv\"\n",
    "\n",
    "    # output_file = './T3BodyLenRank/Data231202_newDATA_rank_Score_new.csv'  # 替换为你的输出文件路径\n",
    "    # df.to_csv(output_file, index=False)\n",
    "    df.to_csv(T3_file_name_1, index=False)\n",
    "\n",
    "\n",
    "    # 只保留需要的列，并保存到新的CSV文件\n",
    "    T3_final_df = df[['id','title', 'body_len', 'T3_rank','T3_S_scale', 'T3_S_score']]\n",
    "    # T3_final_df.to_csv('./T3BodyLenRank/Data231202_T3scoreResult.csv', index=False)\n",
    "    T3_final_df.to_csv(T3_file_name_2, index=False)\n",
    "\n",
    "    print(\"处理完成，并将结果保存到新的CSV文件中。\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 加载孟加拉语模型\n",
    "    nlp = bn_core_news_sm.load()\n",
    "    # # textrank算法计算权重\n",
    "    # update 3.9：改进版！！\n",
    "    def textrank_weighted_word_graph(merged_titles):\n",
    "        tokens = nlp(merged_titles) # 分词\n",
    "        print(len(tokens))\n",
    "        # print(tokens)\n",
    "\n",
    "        graph = nx.Graph()\n",
    "        window_size = 80  # 根据需要调整窗口大小\n",
    "\n",
    "        for i, token in enumerate(tokens):\n",
    "            for j in range(i+1, min(i+window_size+1, len(tokens))):\n",
    "                if token != tokens[j]:  # 添加边,避免自环\n",
    "                    if graph.has_edge(token, tokens[j]):\n",
    "                        graph[token][tokens[j]]['weight'] += 1 #在添加边时,先检查边是否已经存在。如果边已经存在,则将权重加1;否则,添加一个新边,权重为1。这样可以避免重复添加边。\n",
    "                    else:\n",
    "                        graph.add_edge(token, tokens[j], weight=1)\n",
    "\n",
    "        # 使用NetworkX的PageRank算法计算每个节点（词）的权重\n",
    "        pagerank_scores = nx.pagerank(graph, weight='weight')\n",
    "\n",
    "        return pagerank_scores,graph\n",
    "\n",
    "    # 读取CSV文件并合并所有标题\n",
    "    df = data_ORI\n",
    "\n",
    "    merged_titles = ' '.join(title.strip() for title in df['title'])\n",
    "\n",
    "    # ====================================\n",
    "    # 获取孟加拉语的停用词列表\n",
    "    stop_words = set(stopwords.words('bengali'))\n",
    "    # print(stop_words)\n",
    "\n",
    "    # 自定义标点符号列表\n",
    "    custom_punctuation = ['‘', '’']\n",
    "\n",
    "    # 合并 NLTK 提供的标点符号列表和自定义标点符号列表\n",
    "    all_punctuation = string.punctuation + ''.join(custom_punctuation)\n",
    "\n",
    "    print(all_punctuation)\n",
    "    # 分词# word_tokens = word_tokenize(merged_titles)\n",
    "\n",
    "    word_tokens = nlp(merged_titles) # 分词\n",
    "    # word_tokens = merged_titles.split() # 根据空格分词\n",
    "    token_texts = [token.text.strip() for token in word_tokens] # 去除多余空格\n",
    "\n",
    "    # print(token_texts)\n",
    "    print(type(token_texts))\n",
    "\n",
    "\n",
    "\n",
    "    # 去除停用词\n",
    "    # filtered_titles = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_titles = [w for w in token_texts if not w in stop_words] # 去除停用词\n",
    "    filtered_titles = [word for word in filtered_titles if word not in all_punctuation] # 去除标点符号\n",
    "\n",
    "    print(\"filtered_titles len\\n\",len(filtered_titles)) # 字符串数量！\n",
    "\n",
    "    # 将去除停用词后的词重新组合成字符串\n",
    "    filtered_titles_text = ' '.join(filtered_titles)\n",
    "\n",
    "    print(len(filtered_titles_text)) # 字符串长度！别被误导（所少个字符）\n",
    "    # ====================================\n",
    "\n",
    "    # 计算词权重\n",
    "    word_weights,graph = textrank_weighted_word_graph(filtered_titles_text)\n",
    "\n",
    "    # 保存pagerank算法后的词关系权重 可视化\n",
    "    # 根据PageRank值更新边的权重\n",
    "    # 记录权重关系 字典形式存储\n",
    "    pagerank_weighted_graph = nx.Graph()\n",
    "    for node, score in word_weights.items():\n",
    "        pagerank_weighted_graph.add_node(node)\n",
    "\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        weight = data['weight'] * word_weights[u] * word_weights[v]\n",
    "        pagerank_weighted_graph.add_edge(u, v, weight=weight)\n",
    "\n",
    "    graph_content_file_name = f\"./T4TitleTextRank/{model_CLS_name}/T4_{date_UNI}_{model_CLS_name}_graph_content.txt\"\n",
    "    with open('./T4TitleTextRank/graph_content.txt', 'w') as file:\n",
    "        file.write(str(nx.to_dict_of_dicts(pagerank_weighted_graph)))\n",
    "\n",
    "    sorted_words = sorted(word_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 保存到新的CSV文件\n",
    "    # word_weights_df = pd.DataFrame(word_weights.items(), columns=['word', 'weight'])\n",
    "    word_weights_df = pd.DataFrame(sorted_words, columns=['word', 'weight'])\n",
    "\n",
    "    word_weight_file_name = f\"./T4TitleTextRank/{model_CLS_name}/T4_{date_UNI}_{model_CLS_name}_word_weight_new.csv\"\n",
    "\n",
    "    # word_weights_df.to_csv('./T4TitleTextRank/word_weight.csv', index=False)\n",
    "    # word_weights_df.to_csv('./T4TitleTextRank/word_weight_new.csv', index=False)\n",
    "    word_weights_df.to_csv(word_weight_file_name, index=False)\n",
    "\n",
    "    # 接下来，计算每个标题的权重\n",
    "    # 读取词权重文件\n",
    "    # word_weights_df = pd.read_csv('./T4TitleTextRank/word_weight.csv')\n",
    "    # word_weights_df = pd.read_csv('./T4TitleTextRank/word_weight_new.csv')\n",
    "    word_weights_df = pd.read_csv(word_weight_file_name)\n",
    "\n",
    "\n",
    "    # 将词权重转换为字典，方便查找\n",
    "    word_weights = pd.Series(word_weights_df.weight.values, index=word_weights_df.word).to_dict()\n",
    "\n",
    "    # print(word_weights)\n",
    "    # 读取新闻标题文件\n",
    "    titles_df = data_ORI\n",
    "    # titles_df = pd.read_csv('./Data231202-231211/Data231202.csv')\n",
    "    # titles_df = titles_df['title']\n",
    "\n",
    "\n",
    "\n",
    "    # 定义一个函数，用于计算标题的权重\n",
    "    def calculate_title_weight(title):\n",
    "        doc = nlp(title)\n",
    "        # 对标题进行分词并计算总权重\n",
    "        return sum(word_weights.get(token.text, 0) for token in doc)  # 如果词不在word_weights中，则默认权重为0\n",
    "        # return sum(word_weights.get(token.text, 0) for token in doc if token.text not in stop_words and token.text not in all_punctuation)  # 如果词不在word_weights中，则默认权重为0\n",
    "        # return sum(word_weights.get(token.text, 0) for token in doc if token.text not in stop_words and token.text not in string.punctuation)  # 如果词不在word_weights中，则默认权重为0\n",
    "\n",
    "\n",
    "    # 计算每个标题的权重\n",
    "    titles_df['T4_title_weight'] = titles_df['title'].apply(calculate_title_weight)\n",
    "    # print(titles_df['T4_title_weight'])\n",
    "\n",
    "    # 根据权重排序并添加排名，相同权重的排名相同\n",
    "    titles_df = titles_df.sort_values(by='T4_title_weight', ascending=False)\n",
    "    titles_df['T4_rank'] = titles_df['T4_title_weight'].rank(method='min', ascending=False)\n",
    "\n",
    "    # 对权重进行归一化处理，并存储结果到\"S_scale\"列\n",
    "    scaler = MinMaxScaler()\n",
    "    titles_df['T4_S_scale'] = scaler.fit_transform(titles_df[['T4_title_weight']])  # 归一化映射到分数！\n",
    "\n",
    "    # # 创建\"S_score\"列\n",
    "    # titles_df['T4_S_score'] = titles_df['T4_S_scale'] * 20\n",
    "\n",
    "    # 创建\"S_score\"列\n",
    "    titles_df['T4_S_score'] = titles_df['T4_S_scale'] * 100\n",
    "\n",
    "    # 只保留需要的列\n",
    "    T4_final_df = titles_df[['id','title', 'T4_title_weight', 'T4_rank', 'T4_S_scale', 'T4_S_score']]\n",
    "\n",
    "\n",
    "    # 保存到新的csv文件\n",
    "    # final_df.to_csv('./T4TitleTextRank/titles_weight.csv', index=False)\n",
    "\n",
    "    T4_file_name = f\"./T4TitleTextRank/{model_CLS_name}/T4_{date_UNI}_{model_CLS_name}_result_new.csv\"\n",
    "    # T4_final_df.to_csv('./T4TitleTextRank/titles_weight_new.csv', index=False)\n",
    "    T4_final_df.to_csv(T4_file_name, index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 提取新闻的category1进行类别评分\n",
    "\n",
    "    # category_df = pd.read_csv('./T5CateforyScore/category_score.csv')\n",
    "    category_df = pd.read_csv('./T5CateforyScore/category_score_FIX.csv')\n",
    "\n",
    "\n",
    "    # Load the CSV file with news data\n",
    "    # news_df = pd.read_csv('./Data231202-231211_FIX/Data231202_newDATA.csv')\n",
    "    news_df = data_ORI\n",
    "\n",
    "\n",
    "    # Merge the two DataFrames based on the \"category1\" column\n",
    "    merged_df = pd.merge(news_df, category_df, how='left', left_on='category1', right_on='category')\n",
    "\n",
    "    # Sort the merged DataFrame based on the \"rank\" column\n",
    "    sorted_df = merged_df.sort_values(by='T5_rank')\n",
    "\n",
    "    # Select the desired columns\n",
    "    selected_columns = ['id','title', 'category1', 'T5_rank', 'T5_S_scale', 'T5_S_score']\n",
    "    T5_final_df = sorted_df[selected_columns]\n",
    "\n",
    "    T5_file_name = f\"./T5CateforyScore/{model_CLS_name}/T5_{date_UNI}_{model_CLS_name}_result_new.csv\"\n",
    "    # Save the result to a new CSV file\n",
    "    # T5_final_df.to_csv('./T5CateforyScore/Data231202_categoryScore_new.csv', index=False)\n",
    "    T5_final_df.to_csv(T5_file_name, index=False)\n",
    "\n",
    "\n",
    "\n",
    "    # T1_final_df :'id','T1_ori_indexFrom0', 'title', 'body', 'T1_clus_news_num', 'T1_rank','T1_S_scale', 'T1_S_score'\n",
    "    # T2_final_df:'id','website_id', 'title', 'T2_S_score', 'T2_rank'\n",
    "    # T3_final_df:'id','title', 'body_len', 'T3_rank','T3_S_scale', 'T3_S_score'\n",
    "    # T4_final_df: 'id','title', 'T4_title_weight', 'T4_rank', 'T4_S_scale', 'T4_S_score'\n",
    "    # T5_final_df:'id','title', 'category1', 'T5_rank', 'T5_S_scale', 'T5_S_score'\n",
    "    # 合并5个dataframe：\n",
    "    # 第一步:将T1_final_df和T2_final_df合并\n",
    "    merged_df = pd.merge(T1_final_df, T2_final_df, on=['id', 'title'], how='outer')\n",
    "\n",
    "    # 第二步:将第一步合并后的DataFrame与T3_final_df合并\n",
    "    merged_df = pd.merge(merged_df, T3_final_df, on=['id', 'title'], how='outer')\n",
    "\n",
    "    # 第三步:将第二步合并后的DataFrame与T4_final_df合并\n",
    "    merged_df = pd.merge(merged_df, T4_final_df, on=['id', 'title'], how='outer')\n",
    "\n",
    "    # 第四步:将第三步合并后的DataFrame与T5_final_df合并\n",
    "    merged_df = pd.merge(merged_df, T5_final_df, on=['id', 'title'], how='outer')\n",
    "\n",
    "    # 打印合并后的 DataFrame\n",
    "    Merge_file_name = f\"./MergeFiveDScore/{model_CLS_name}/Merge_{date_UNI}_{model_CLS_name}_FiveDScore_result_new.csv\"\n",
    "    # merged_df.to_csv('./MergeFiveDScore/FiveDScore_Merge.csv', index=False)\n",
    "    merged_df.to_csv(Merge_file_name, index=False)\n",
    "\n",
    "    # print(merged_df)\n",
    "\n",
    "\n",
    "    # 假设权重 \n",
    "    # w1, w2, w3, w4, w5 = 0.5,0.05,0.05,0.3,0.1\n",
    "    # 权重设置思路：\n",
    "    # ①层次分析法 根据各任务的重要性赋权\n",
    "    # ②迭代 需要一个评估指标（正确个数？）来进行迭代找出模型最优权重！\n",
    "\n",
    "    # 层次分析法权重！：\n",
    "    # 通过进行层次分析法确定的五个维度权重为:相似新闻报道频率(0.46221)、新闻来源网站权威性(0.03503)、新闻标题重要性(0.35029)、新闻正文长度(0.03049)、新闻类别(0.12198)。\n",
    "    # 对应\n",
    "    # T1：0.46221 相似新闻---clusterScore\n",
    "    # T2: 0.03503 网站权威性---WebsiteRank \n",
    "    # T3：0.03049 正文长度---bodyLenRank\n",
    "    # T4：0.35029 新闻标题重要性 --- TitleTextRank\n",
    "    # T5：0.12198 新闻类别 --- Category\n",
    "\n",
    "    w1, w2, w3, w4, w5 = 0.46221, 0.03503, 0.03049, 0.35029, 0.12198\n",
    "\n",
    "\n",
    "    # 计算总分数\n",
    "    merged_df['total_S_score'] = w1 * merged_df['T1_S_score'] + w2 * merged_df['T2_S_score'] + w3 * merged_df['T3_S_score'] + w4 * merged_df['T4_S_score'] + w5 * merged_df['T5_S_score']\n",
    "\n",
    "    # 生成排名\n",
    "    merged_df['total_rank'] = merged_df['total_S_score'].rank(method='min', ascending=False)\n",
    "\n",
    "    # 根据总分数降序排序\n",
    "    merged_df = merged_df.sort_values('total_S_score', ascending=False)\n",
    "\n",
    "    # 将结果保存到csv文件\n",
    "    total_result_file_name = f\"./MergeFiveDScore/{model_CLS_name}/total_result_{date_UNI}_{model_CLS_name}.csv\"\n",
    "    # merged_df.to_csv('./MergeFiveDScore/total_result.csv', index=False)\n",
    "    merged_df.to_csv(total_result_file_name , index=False)\n",
    "\n",
    "\n",
    "    selected_columns = ['id','T1_ori_indexFrom0', 'category1','title','body','total_S_score','total_rank']\n",
    "    merged_df_pure =  merged_df[selected_columns]\n",
    "\n",
    "    total_result_pure_file_name = f\"./MergeFiveDScore/{model_CLS_name}/total_result_pure_{date_UNI}_{model_CLS_name}.csv\"\n",
    "\n",
    "    # Save the result to a new CSV file\n",
    "    # merged_df_pure.to_csv('./MergeFiveDScore/total_result_pure.csv', index=False)\n",
    "    merged_df_pure.to_csv(total_result_pure_file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becadd2c-c34c-4cfb-8a70-bb9e3fdd2444",
   "metadata": {},
   "outputs": [],
   "source": [
    "===================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f641b0f9-0da3-4e2a-8754-865e0e2588ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 查询数据库中2024-03-02 ~ 2024-03-18的数据用于评估\n",
    "# todo：包装成一个函数 供外部调用 存储路径和SQL改善\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# date_sel要处理成20xx-01-01格式\n",
    "# date_sel要处理成20xx-01-01格式\n",
    "def DateDataGET_Impl(date_sel):\n",
    "    # 检查文件夹是否存在，如果不存在则创建\n",
    "    date_sel_folder = f\"./datasets/mbert_BiLSTM_SelfAttention/{date_sel}\"\n",
    "    if not os.path.exists(date_sel_folder):\n",
    "        os.makedirs(date_sel_folder)\n",
    "\n",
    "    \n",
    "    output_file_path = f\"{date_sel_folder}/news_{date_sel}.csv\"\n",
    "    # 缓存处理 不存在才需要重新查询保存\n",
    "    if not os.path.exists(output_file_path):\n",
    "        # df.to_csv(output_file_path, index=False)\n",
    "        # 连接数据库\n",
    "        conn = mysql.connector.connect(\n",
    "          host=\"172.16.234.200\",\n",
    "          user=\"dg_news\",\n",
    "          password=\"dg_news\",\n",
    "          database=\"dg_crawler\"\n",
    "        )\n",
    "\n",
    "\n",
    "        # 执行SQL查询\n",
    "        query = \"\"\"\n",
    "            SELECT *\n",
    "            FROM news\n",
    "            WHERE pub_time='\"\"\"+date_sel+\"\"\"' and news.language_id=1779\n",
    "        \"\"\"\n",
    "\n",
    "        # 将查询结果存入DataFrame\n",
    "        df = pd.read_sql(query, conn)\n",
    "\n",
    "        # 关闭数据库连接\n",
    "        conn.close()\n",
    "\n",
    "        # 将DataFrame写入CSV文件\n",
    "        # 按天文件夹\n",
    "\n",
    "#         # 检查文件夹是否存在，如果不存在则创建\n",
    "#         date_sel_folder = f\"./datasets/mbert_BiLSTM_SelfAttention/{date_sel}\"\n",
    "#         if not os.path.exists(date_sel_folder):\n",
    "#             os.makedirs(date_sel_folder)\n",
    "\n",
    "#         output_file_path = f\"{date_sel_folder}/news_{date_sel}.csv\"\n",
    "        df.to_csv(output_file_path, index=False)\n",
    "              \n",
    "              \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55f8772d-0f39-47c5-8477-1cd32f6f6fc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ===================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0584bdf-4a82-4e04-85f5-4a3e8007330d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dev实验版 所以堆到一起 以上的代码实际中在另一个文件\n",
    "import pandas as pd\n",
    "# from ProcessCode.DateDataGET import DateDataGET_Impl              # 正式版记得打开\n",
    "# from ProcessCode.DateDataProcess import DateDataProcess_Impl     # 正式版记得打开\n",
    "\n",
    "# date_sel要处理成2024-01-01格式\n",
    "def get_merge_data(date_sel):\n",
    "    \n",
    "    model_CLS_name = \"mbert_BiLSTM_SelfAttention\" ###!!!\n",
    "    # 读取total_result_pure.csv文件并提取id列的前20条数据以及对应的total_rank字段\n",
    "    # 总评估处理后文件\n",
    "    total_result_file_name = f\"./MergeFiveDScore/{model_CLS_name}/total_result_pure_{date_sel}_{model_CLS_name}.csv\"\n",
    "    \n",
    "    # 没有才需要进行处理，加快速度\n",
    "    if not os.path.exists(total_result_file_name):\n",
    "        DateDataGET_Impl(date_sel) # 从数据库中获取每日新闻数据 \n",
    "        DateDataProcess_Impl(date_sel) # 处理每日新闻数据获得分类处理后的_processed和总评估处理后的total_result_pure_\n",
    "        \n",
    "    # total_result = pd.read_csv('./datasets/total_result_pure.csv', encoding='utf-8')\n",
    "    total_result = pd.read_csv(total_result_file_name, encoding='utf-8')\n",
    "\n",
    "    # id_and_rank = total_result[['id', 'total_rank']].head(20)\n",
    "    id_and_rank = total_result[['id', 'total_rank']]\n",
    "\n",
    "    # 分类处理后文件\n",
    "    processed_data_file_name = f\"./datasets/{model_CLS_name}/news_{date_sel}_processed_{model_CLS_name}.csv\"\n",
    "\n",
    "    # news_data = pd.read_csv('./datasets/Data231202_processed.csv', encoding='utf-8')\n",
    "    news_data = pd.read_csv(processed_data_file_name, encoding='utf-8')\n",
    "\n",
    "    # 读取website_Rank_new_FIX.csv文件\n",
    "    website_rank = pd.read_csv('./T2WebsiteRank/website_Rank_new_FIX.csv', encoding='utf-8')\n",
    "\n",
    "    # 根据id列表在news_data中读取对应id的数据\n",
    "    merged_data = pd.merge(id_and_rank, news_data, on='id')\n",
    "\n",
    "    # 根据website_id读取website_Rank_new_FIX.csv文件中的url字段\n",
    "    merged_data = pd.merge(merged_data, website_rank, on='website_id')\n",
    "\n",
    "\n",
    "    # 对字符串类型的字段进行编码转换\n",
    "    def encode_utf8(value):\n",
    "        if isinstance(value, str):\n",
    "            return value.encode('utf-8').decode('utf-8')\n",
    "        return value\n",
    "\n",
    "    string_columns = ['url', 'request_url', 'response_url', 'category1', 'category2', 'title', 'abstract', 'body', 'images', 'md5']\n",
    "    merged_data[string_columns] = merged_data[string_columns].applymap(encode_utf8)\n",
    "    \n",
    "    return merged_data\n",
    "\n",
    "\n",
    "\n",
    "def get_data_MIXed_Impl(date_sel):\n",
    "    \n",
    "    merged_data = get_merge_data(date_sel)\n",
    "    # 将每个id对应的数据包装成json格式\n",
    "    result_json = []\n",
    "    for index, row in merged_data.iterrows():\n",
    "        data_dict = {\n",
    "            'artId': row['id'],\n",
    "            'total_rank': row['total_rank'],\n",
    "            'websiteUrl': row['url'],\n",
    "            'website_id': row['website_id'],\n",
    "            'request_url': row['request_url'],\n",
    "            'response_url': row['response_url'],\n",
    "            'artType': row['category1'],\n",
    "            # 'category2': row['category2'],\n",
    "            'artTitle': row['title'],\n",
    "            'abstract': row['abstract'],\n",
    "            'artContent': row['body'],\n",
    "            'artTime': row['pub_time'],\n",
    "            'cole_time': row['cole_time'],\n",
    "            'artImageUrl': row['images'],\n",
    "            'language_id': row['language_id'],\n",
    "            'md5': row['md5'],\n",
    "            'artCusId': 582,\n",
    "            # test\n",
    "            \"customer\": {\"cusId\": 582, \"cusName\": \"admin\",\n",
    "                         \"cusPass\": None,\n",
    "                         \"cusSpider\": \"\",\n",
    "                         \"cusAvatarUrl\": \"http://localhost:8080/img/Man.png\",\n",
    "                         \"cusStyle\": \"这个人很懒, 什么都没写\",\n",
    "                         \"cusGender\": 0,\n",
    "                         \"cusTime\": \"2024-03-14T21:53:09.000+0000\", \"cusLegal\": 0},\n",
    "            \"artFeature\": {\"afcId\": 710, \"afcArtId\": 20171864,\n",
    "                           \"afcLikeNum\": 0, \"afcDislikeNum\": 0,\n",
    "                           \"afcComNum\": 0, \"afcRepNum\": 0,\n",
    "                           \"afcReadNum\": 0,\n",
    "                           \"afcArtTime\": None},\n",
    "            \"cusArtBehavior\": None\n",
    "        }\n",
    "        result_json.append(data_dict)\n",
    "        return result_json\n",
    "\n",
    "\n",
    "def get_data_ByType_Impl(date_sel,artType,page,pageSize):\n",
    "    \n",
    "    merged_data = get_merge_data(date_sel)\n",
    "\n",
    "    \n",
    "    if artType != \"综合\":\n",
    "        # 筛选类别category1为“test”的数据\n",
    "        filtered_data = merged_data[merged_data['category1'] == artType]\n",
    "    else:\n",
    "        filtered_data = merged_data\n",
    "\n",
    "    # 读取时读全部 然后选择时按照pageSize进行选择显示\n",
    "    print(type(pageSize))\n",
    "    filtered_data = filtered_data.head(int(pageSize))\n",
    "\n",
    "    # 将每个id对应的数据包装成json格式\n",
    "    result_json = []\n",
    "    # for index, row in merged_data.iterrows():\n",
    "    for index, row in filtered_data.iterrows():\n",
    "        data_dict = {\n",
    "            'artId': row['id'],\n",
    "            'total_rank': row['total_rank'],\n",
    "            'websiteUrl': row['url'],\n",
    "            'website_id': row['website_id'],\n",
    "            'request_url': row['request_url'],\n",
    "            'response_url': row['response_url'],\n",
    "            'artType': row['category1'],\n",
    "            # 'category2': row['category2'],\n",
    "            'artTitle': row['title'],\n",
    "            'abstract': row['abstract'],\n",
    "            'artContent': row['body'],\n",
    "            'artTime': row['pub_time'],\n",
    "            'cole_time': row['cole_time'],\n",
    "            'artImageUrl': row['images'],\n",
    "            'language_id': row['language_id'],\n",
    "            'md5': row['md5'],\n",
    "            'artCusId': 582,\n",
    "            # test\n",
    "            \"customer\": {\"cusId\": 582, \"cusName\": \"admin\",\n",
    "                         \"cusPass\": None,\n",
    "                         \"cusSpider\": \"\",\n",
    "                         \"cusAvatarUrl\": \"http://localhost:8080/img/Man.png\",\n",
    "                         \"cusStyle\": \"这个人很懒, 什么都没写\",\n",
    "                         \"cusGender\": 0,\n",
    "                         \"cusTime\": \"2024-03-14T21:53:09.000+0000\", \"cusLegal\": 0},\n",
    "            \"artFeature\": {\"afcId\": 710, \"afcArtId\": 20171864,\n",
    "                           \"afcLikeNum\": 0, \"afcDislikeNum\": 0,\n",
    "                           \"afcComNum\": 0, \"afcRepNum\": 0,\n",
    "                           \"afcReadNum\": 0,\n",
    "                           \"afcArtTime\": None},\n",
    "            \"cusArtBehavior\": None\n",
    "        }\n",
    "        result_json.append(data_dict)\n",
    "\n",
    "    return result_json\n",
    "\n",
    "\n",
    "def get_Categorys_Impl(date_sel):\n",
    "    merged_data = get_merge_data(date_sel)\n",
    "\n",
    "    # # 统计category1不同类别的种类数\n",
    "    # category_counts = merged_data['category1'].value_counts().to_dict()\n",
    "\n",
    "    # 提取category1列并转换为列表，并去重\n",
    "    category_list = merged_data['category1'].unique().tolist()\n",
    "\n",
    "    # 在列表的最前面加入一个元素\"综合\"\n",
    "    category_list.insert(0, '综合')\n",
    "\n",
    "    return category_list\n",
    "\n",
    "\n",
    "def get_ArtMain_Impl(date_sel,artId):\n",
    "    \n",
    "    merged_data = get_merge_data(date_sel)\n",
    "\n",
    "    # 根据artId筛选数据\n",
    "    filtered_data = merged_data[merged_data['id'] == int(artId)]\n",
    "\n",
    "    # 将每个id对应的数据包装成json格式\n",
    "    \n",
    "    result_dict = {}\n",
    "    # for index, row in merged_data.iterrows():\n",
    "    for index, row in filtered_data.iterrows():\n",
    "        data_dict = {\n",
    "            'artId': row['id'],\n",
    "            'total_rank': row['total_rank'],\n",
    "            'websiteUrl': row['url'],\n",
    "            'website_id': row['website_id'],\n",
    "            'request_url': row['request_url'],\n",
    "            'response_url': row['response_url'],\n",
    "            'artType': row['category1'],\n",
    "            # 'category2': row['category2'],\n",
    "            'artTitle': row['title'],\n",
    "            'abstract': row['abstract'],\n",
    "            'artContent': row['body'],\n",
    "            'artTime': row['pub_time'],\n",
    "            'cole_time': row['cole_time'],\n",
    "            'artImageUrl': row['images'],\n",
    "            'language_id': row['language_id'],\n",
    "            'md5': row['md5'],\n",
    "            'artCusId': 582,\n",
    "            # test\n",
    "            \"customer\": {\"cusId\": 582, \"cusName\": \"admin\",\n",
    "                         \"cusPass\": None,\n",
    "                         \"cusSpider\": \"\",\n",
    "                         \"cusAvatarUrl\": \"http://localhost:8080/img/Man.png\",\n",
    "                         \"cusStyle\": \"这个人很懒, 什么都没写\",\n",
    "                         \"cusGender\": 0,\n",
    "                         \"cusTime\": \"2024-03-14T21:53:09.000+0000\", \"cusLegal\": 0},\n",
    "            \"artFeature\": {\"afcId\": 710, \"afcArtId\": 20171864,\n",
    "                           \"afcLikeNum\": 0, \"afcDislikeNum\": 0,\n",
    "                           \"afcComNum\": 0, \"afcRepNum\": 0,\n",
    "                           \"afcReadNum\": 0,\n",
    "                           \"afcArtTime\": None},\n",
    "            \"cusArtBehavior\": None\n",
    "        }\n",
    "\n",
    "        result_dict = data_dict\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "# 右侧推荐新闻处 随机返回pageSize条新闻\n",
    "def get_randomArt_Impl(date_sel,page,pageSize):\n",
    "    \n",
    "    merged_data = get_merge_data(date_sel)\n",
    "\n",
    "        \n",
    "    # 随机选取数据\n",
    "    # print(min(int(pageSize), len(merged_data)))\n",
    "    sample_data = merged_data.sample(n=min(int(pageSize), len(merged_data)))\n",
    "\n",
    "    # 将每个id对应的数据包装成json格式\n",
    "    \n",
    "    result_json = []\n",
    "    # for index, row in merged_data.iterrows():\n",
    "    for index, row in sample_data.iterrows():\n",
    "        data_dict = {\n",
    "            'artId': row['id'],\n",
    "            'total_rank': row['total_rank'],\n",
    "            'websiteUrl': row['url'],\n",
    "            'website_id': row['website_id'],\n",
    "            'request_url': row['request_url'],\n",
    "            'response_url': row['response_url'],\n",
    "            'artType': row['category1'],\n",
    "            # 'category2': row['category2'],\n",
    "            'artTitle': row['title'],\n",
    "            'abstract': row['abstract'],\n",
    "            'artContent': row['body'],\n",
    "            'artTime': row['pub_time'],\n",
    "            'cole_time': row['cole_time'],\n",
    "            'artImageUrl': row['images'],\n",
    "            'language_id': row['language_id'],\n",
    "            'md5': row['md5'],\n",
    "            'artCusId': 582,\n",
    "            # test\n",
    "            \"customer\": {\"cusId\": 582, \"cusName\": \"admin\",\n",
    "                         \"cusPass\": None,\n",
    "                         \"cusSpider\": \"\",\n",
    "                         \"cusAvatarUrl\": \"http://localhost:8080/img/Man.png\",\n",
    "                         \"cusStyle\": \"这个人很懒, 什么都没写\",\n",
    "                         \"cusGender\": 0,\n",
    "                         \"cusTime\": \"2024-03-14T21:53:09.000+0000\", \"cusLegal\": 0},\n",
    "            \"artFeature\": {\"afcId\": 710, \"afcArtId\": 20171864,\n",
    "                           \"afcLikeNum\": 0, \"afcDislikeNum\": 0,\n",
    "                           \"afcComNum\": 0, \"afcRepNum\": 0,\n",
    "                           \"afcReadNum\": 0,\n",
    "                           \"afcArtTime\": None},\n",
    "            \"cusArtBehavior\": None\n",
    "        }\n",
    "        result_json.append(data_dict)\n",
    "        # print(result_json)\n",
    "\n",
    "    return result_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff43405-ad69-4ad2-b2c0-6b32ff3d37ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "===================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c017b2cb-30f5-4d78-9306-704b93e1b937",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-21\n",
      "20606864\n",
      "处理完成！！\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [2024-04-03 17:14:20] \"GET /main?dateSel=2024-03-21&artId=20606864 HTTP/1.1\" 200 49684 0.591098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-21\n",
      "0\n",
      "6\n",
      "处理完成！！\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [2024-04-03 17:14:36] \"GET /random_art?dateSel=2024-03-21&page=0&pageSize=6 HTTP/1.1\" 200 125536 0.128292\n"
     ]
    }
   ],
   "source": [
    "# dev版 以上实际上在另一个文件\n",
    "# coding=utf8\n",
    "import pandas as pd\n",
    "from flask import *\n",
    "from flask_cors import CORS  # 导入CORS模块\n",
    "from gevent import pywsgi\n",
    "# from ProcessCode.survicesImpl import get_data_MIXed_Impl,get_data_ByType_Impl,get_Categorys_Impl,get_randomArt_Impl,get_ArtMain   正式版记得打开这个！！！！\n",
    "\n",
    "# 使用Flask创建算法接口\n",
    "app = Flask(__name__)\n",
    "# app.json.ensure_ascii = False  # 解决中文乱码问题  flask版本2.3.0以上使用\n",
    "app.config['JSON_AS_ASCII'] = False  # 解决中文乱码问题  flask版本 2.3.0以下使用\n",
    "\n",
    "CORS(app)  # 添加跨域支持\n",
    "\n",
    "# 获取“综合”类别的重要新闻\n",
    "@app.route('/get_data_mixed', methods=['GET'])\n",
    "def get_data_MIXed():\n",
    "\n",
    "    # 获取 URL 参数中的参数值\n",
    "    date_sel =  request.args.get('dateSel')\n",
    "    artType = request.args.get('artType')\n",
    "    page = request.args.get('page')\n",
    "    pageSize = request.args.get('pageSize')\n",
    "\n",
    "    result_json = get_data_MIXed_Impl(date_sel)\n",
    "    print(\"处理完成！！\")\n",
    "\n",
    "    response = jsonify(result_json)\n",
    "    response.headers['Content-Type'] = 'application/json; charset=utf-8'\n",
    "    return response  # 将字节串解码为UTF-8字符串\n",
    "\n",
    "\n",
    "# 按类别获取类别的重要新闻（包括“综合”）\n",
    "@app.route('/get_data_by_type', methods=['GET'])\n",
    "def get_data_ByType():\n",
    "    # 获取 URL 参数中的参数值\n",
    "    date_sel =  request.args.get('dateSel') # 获取日期参数\n",
    "\n",
    "    artType = request.args.get('artType')\n",
    "    page = request.args.get('page')\n",
    "    pageSize = request.args.get('pageSize')\n",
    "    \n",
    "    print(date_sel)\n",
    "    print(artType)\n",
    "    print(page)\n",
    "    print(pageSize)\n",
    "\n",
    "    # result_json = get_data_ByType_Impl(artType,page,pageSize)\n",
    "    result_json = get_data_ByType_Impl(date_sel,artType,page,pageSize)\n",
    "\n",
    "    response = jsonify(result_json)\n",
    "    print(\"处理完成！！\")\n",
    "\n",
    "    # response = jsonify(test_json)\n",
    "    response.headers['Content-Type'] = 'application/json; charset=utf-8'\n",
    "    # return response.data.decode('utf-8')  # 将字节串解码为UTF-8字符串\n",
    "    return response  # 将字节串解码为UTF-8字符串\n",
    "\n",
    "@app.route('/type', methods=['GET'])\n",
    "def get_Categorys():\n",
    "    date_sel =  request.args.get('dateSel') # 获取日期参数\n",
    "    print(date_sel)\n",
    "    \n",
    "    category_list = get_Categorys_Impl(date_sel)\n",
    "    print(\"处理完成！！\")\n",
    "\n",
    "    return jsonify(category_list)\n",
    "\n",
    "# 返回完整文章内容\n",
    "@app.route('/main', methods=['GET'])\n",
    "def get_ArtMain():\n",
    "    \n",
    "    date_sel =  request.args.get('dateSel') # 获取日期参数\n",
    "\n",
    "    # 获取artId参数的值\n",
    "    artId = request.args.get('artId')\n",
    "\n",
    "    print(date_sel)\n",
    "    print(artId)\n",
    "    # result_dict = get_ArtMain(artId)\n",
    "    result_dict = get_ArtMain_Impl(date_sel,artId) # FIX\n",
    "    print(\"处理完成！！\")\n",
    "\n",
    "    response = jsonify(result_dict)\n",
    "    response.headers['Content-Type'] = 'application/json; charset=utf-8'\n",
    "    return response  # 将字节串解码为UTF-8字符串\n",
    "\n",
    "\n",
    "# 右侧推荐新闻处 随机返回pageSize条新闻\n",
    "@app.route('/random_art', methods=['GET'])\n",
    "def get_randomArt():\n",
    "    \n",
    "    date_sel =  request.args.get('dateSel') # 获取日期参数\n",
    "\n",
    "    # 获取 URL 参数中的参数值\n",
    "    # artType = request.args.get('artType')\n",
    "    page = request.args.get('page')\n",
    "    pageSize = request.args.get('pageSize')\n",
    "    print(date_sel)\n",
    "\n",
    "    print(page)\n",
    "    print(pageSize)\n",
    "\n",
    "    # result_json = get_randomArt_Impl(page,pageSize)\n",
    "    result_json = get_randomArt_Impl(date_sel,page,pageSize) # FIX\n",
    "    \n",
    "    print(\"处理完成！！\")\n",
    "\n",
    "\n",
    "    response = jsonify(result_json)\n",
    "    response.headers['Content-Type'] = 'application/json; charset=utf-8'\n",
    "    return response  # 将字节串解码为UTF-8字符串\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # app.run(debug=True)\n",
    "    server = pywsgi.WSGIServer(('127.0.0.1', 5399), app)\n",
    "    server.serve_forever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118b9d5c-0d0c-42f9-9a85-a7036b09306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 17:19调试成功！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-angle",
   "language": "python",
   "name": "conda-angle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
